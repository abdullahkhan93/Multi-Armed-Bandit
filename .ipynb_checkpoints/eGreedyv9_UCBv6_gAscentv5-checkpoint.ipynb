{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eGreedy(k, rounds, epsilon, rewards, initialQ):  \n",
    "    # Reward estimates for each bandit/website\n",
    "    Q = np.full(k, initialQ, dtype = float)\n",
    "    # Reward at specific trial 'n' for each bandit\n",
    "    R = np.full(k, -1)\n",
    "    # Cumulative reward for each bandit/website\n",
    "    TR = np.zeros(k)\n",
    "    # Counts for each bandit/website being chosen\n",
    "    n = np.zeros(k)\n",
    "    # Reward probability for each bandit (Real values unknown to the learner)\n",
    "    rewardProb = rewards\n",
    "    # Percentages of choice of optimal action after each round\n",
    "    optimalActionRatio = np.zeros(rounds)\n",
    "    # Average of total reward\n",
    "    avgReward = np.zeros(rounds)\n",
    "    for i in range(rounds):\n",
    "        if i == 0:\n",
    "            # First trail/iteration\n",
    "            index = np.random.choice(k)\n",
    "        else:\n",
    "            # Random number to decide between exploration and exploitation (choice = 1 (exploit), choice = 0 (explore))\n",
    "            choice = np.random.binomial(1, p = (1 - epsilon))\n",
    "            # Index is used for choosing a specific bandit/website out of the available options\n",
    "            index = 0\n",
    "            if choice == 1:            \n",
    "                # Exploit the best option\n",
    "                index = np.argmax(Q)\n",
    "            else:\n",
    "                # Explore all bandits/websites\n",
    "                index = np.random.choice(k)\n",
    "        # Update the count for the chosen bandit\n",
    "        n[index] += 1\n",
    "        # Reward = 1 if random number > bandit reward probability else 0\n",
    "        reward = np.random.binomial(1, p = rewardProb[index])\n",
    "        R[index] = reward \n",
    "        # Calculate new estimate and cumulative reward for the bandit\n",
    "        # Formula: Q_n+1 = Q_n + [R_n - Q_n] * (1 / n)\n",
    "        Q[index] = Q[index] + (R[index] - Q[index]) / (i + 1)   \n",
    "        # Update values for the bandit/website\n",
    "        TR[index] += R[index]\n",
    "        # Save ratio of optimal action being selected at each round to the total rounds that have taken place\n",
    "        optimalActionRatio[i] = (n[np.argmax(rewardProb)] / (i + 1))\n",
    "        # Update average reward\n",
    "        avgReward[i] = (np.sum(TR) / (i + 1))\n",
    "    return Q, n, TR, optimalActionRatio, avgReward\n",
    "\n",
    "\n",
    "def UCB(k, rounds, c, rewards, initialQ):    \n",
    "    # Reward estimates for each bandit/website\n",
    "    Q = np.full(k, initialQ, dtype = float)\n",
    "    # Reward at specific trial 'n' for each bandit\n",
    "    R = np.full(k, -1)\n",
    "    # Cumulative reward for each bandit/website\n",
    "    TR = np.zeros(k)\n",
    "    # Counts for each bandit/website being chosen\n",
    "    n = np.zeros(k)\n",
    "    # Reward probability for each bandit (Real values unknown to the learner)\n",
    "    rewardProb = rewards\n",
    "    # Percentages of choice of optimal action after each round\n",
    "    optimalActionRatio = np.zeros(rounds)\n",
    "    # Average of total reward\n",
    "    avgReward = np.zeros(rounds)\n",
    "    for i in range(rounds):\n",
    "        # Check for maximisizing bandits/websites\n",
    "        maxIndex = np.where(n == 0)[0]\n",
    "        # Check if a bandit/website hasn't been chosen even once and select it\n",
    "        if len(maxIndex) >= 1:\n",
    "            # Index is used for choosing a specific bandit/website out of the available options\n",
    "            index = maxIndex[0]\n",
    "        else:\n",
    "            # Storage of UCB values for selecting best arm\n",
    "            A = np.zeros(k)\n",
    "            A = Q + c * np.sqrt(np.log(i + 1) / n)\n",
    "            index = np.argmax(A)\n",
    "        # Update the count for the chosen bandit\n",
    "        n[index] += 1\n",
    "        # Reward = 1 if random number > bandit reward probability else 0\n",
    "        reward = np.random.binomial(1, p = rewardProb[index])   \n",
    "        R[index] = reward\n",
    "        # Formula: Q_n+1 = Q_n + [R_n - Q_n] * (1 / n)\n",
    "        Q[index] = Q[index] + (R[index] - Q[index])  / (i + 1)\n",
    "        # Update values for the bandit/website\n",
    "        TR[index] += R[index]\n",
    "        # Save ratio of optimal action being selected at each round to the total rounds that have taken place\n",
    "        optimalActionRatio[i] = (n[np.argmax(rewardProb)] / (i + 1))\n",
    "        # Update average reward\n",
    "        avgReward[i] = (np.sum(TR) / (i + 1))\n",
    "    return Q, n, TR, optimalActionRatio, avgReward\n",
    "\n",
    "\n",
    "def gradientAscent(k, rounds, rewards, alpha):    \n",
    "    # Possible actions\n",
    "    actions = np.arange(k)\n",
    "    # Learning preference for each bandit/website\n",
    "    H = np.zeros(k)\n",
    "    # Probabaility of choosing bandit/website at specific time \n",
    "    pieProb = np.exp(H - np.max(H)) / np.sum(np.exp(H - np.max(H)), axis = 0)\n",
    "    # Reward at specific trial 'n' for each bandit\n",
    "    R = np.zeros(k)\n",
    "    # Cumulative reward for each bandit/website\n",
    "    TR = np.zeros(k)\n",
    "    # Counts for each bandit/website being chosen\n",
    "    n = np.zeros(k)\n",
    "    # Reward probability for each bandit (Real values unknown to the learner)\n",
    "    rewardProb = rewards\n",
    "    # Percentages of choice of optimal action after each round\n",
    "    optimalActionRatio = np.zeros(rounds)\n",
    "    # Average of total reward\n",
    "    avgReward = np.zeros(rounds)\n",
    "    for i in range(rounds):\n",
    "        # Choose bandit with the most probability preference\n",
    "        index = np.random.choice(actions, p = pieProb)\n",
    "        not_index = actions != index\n",
    "        # Reward = 1 if random number > bandit reward probability else 0\n",
    "        reward = np.random.binomial(1, p = rewardProb[index]) \n",
    "        R[index] = reward\n",
    "        TR[index] += R[index]\n",
    "        n[index] += 1\n",
    "        # Update average reward\n",
    "        avgReward[i] = (np.sum(TR) / (i + 1))\n",
    "        # Update preference of chosen bandit/website and the rest of them\n",
    "        # Formula for chosen bandit: H_t+1 = H_t + (R_t - mean(R_t) * (1 - pieProb) * alpha (alpha = stepsize)\n",
    "        H[index] = H[index] + (R[index] - avgReward[i]) * (1 - pieProb[index]) * alpha\n",
    "        # Formula for rest of the bandits: H_t+1 = H_t - (R_t - mean(R_t) * pieProb * alpha\n",
    "        H[not_index] = H[not_index] - (R[index] - avgReward[i]) * (pieProb[not_index]) * alpha\n",
    "        # Update probabilities of preference\n",
    "        pieProb = np.exp(H - np.max(H)) / np.sum(np.exp(H - np.max(H)), axis = 0)\n",
    "        # Save ratio of optimal action being selected at each round to the total rounds that have taken place\n",
    "        optimalActionRatio[i] = (n[np.argmax(rewardProb)] / (i + 1))\n",
    "    return H, n, TR, optimalActionRatio, avgReward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "experiments = 100\n",
    "\n",
    "# Variables for experiment 1 setting (eGreedy)\n",
    "rewards = [0.045, 0.12, 0.077, 0.012, 0.15]\n",
    "epsilon = 0.1\n",
    "initialQ = 0\n",
    "rounds = 10000\n",
    "k = 5\n",
    "alpha = 0.1\n",
    "\n",
    "cumulativeRatios1 = np.zeros(rounds)\n",
    "cumulativeAvgRewards1 = np.zeros(rounds)\n",
    "\n",
    "for i in range (experiments):\n",
    "    Q1, n1, TR1, Ratio1, avgRew1 = eGreedy(k, rounds, epsilon, rewards, initialQ)\n",
    "    cumulativeRatios1 = np.add(cumulativeRatios1, Ratio1)\n",
    "    cumulativeAvgRewards1 = np.add(cumulativeAvgRewards1, avgRew1)\n",
    "\n",
    "avgPercentage1 = (cumulativeRatios1 / experiments) * 100\n",
    "avgRewards1 =  cumulativeAvgRewards1 / experiments\n",
    "\n",
    "# Variables for experiment setting 2 (eGreedy)\n",
    "epsilon = 0\n",
    "initialQ = 5\n",
    "\n",
    "cumulativeRatios2 = np.zeros(rounds)\n",
    "cumulativeAvgRewards2 = np.zeros(rounds)\n",
    "\n",
    "for i in range (experiments):\n",
    "    Q2, n2, TR2, Ratio2, avgRew2 = eGreedy(k, rounds, epsilon, rewards, initialQ)\n",
    "    cumulativeRatios2 = np.add(cumulativeRatios2, Ratio2)\n",
    "    cumulativeAvgRewards2 = np.add(cumulativeAvgRewards2, avgRew2)\n",
    "    \n",
    "avgPercentage2 = (cumulativeRatios2 / experiments) * 100\n",
    "avgRewards2 =  cumulativeAvgRewards2 / experiments\n",
    "\n",
    "# Variables for experiment setting 3 (UCB)\n",
    "c = 2\n",
    "initialQ = 0\n",
    "\n",
    "cumulativeRatios3 = np.zeros(rounds)\n",
    "cumulativeAvgRewards3 = np.zeros(rounds)\n",
    "\n",
    "for i in range (experiments):\n",
    "    Q3, n3, TR3, Ratio3, avgRew3 = UCB(k, rounds, c, rewards, initialQ)\n",
    "    cumulativeRatios3 = np.add(cumulativeRatios3, Ratio3)\n",
    "    cumulativeAvgRewards3 = np.add(cumulativeAvgRewards3, avgRew3)\n",
    "\n",
    "avgPercentage3 = (cumulativeRatios3 / experiments) * 100\n",
    "avgRewards3 =  cumulativeAvgRewards3 / experiments\n",
    "\n",
    "# Variables for experiment setting 4 (gradientAscent)\n",
    "\n",
    "cumulativeRatios4 = np.zeros(rounds)\n",
    "cumulativeAvgRewards4 = np.zeros(rounds)\n",
    "\n",
    "for i in range (experiments):\n",
    "    H4, n4, TR4, Ratio4, avgRew4 = gradientAscent(k, rounds, rewards, alpha)\n",
    "    cumulativeRatios4 = np.add(cumulativeRatios4, Ratio4)\n",
    "    cumulativeAvgRewards4 = np.add(cumulativeAvgRewards4, avgRew4)\n",
    "\n",
    "avgPercentage4 = (cumulativeRatios4 / experiments) * 100\n",
    "avgRewards4 =  cumulativeAvgRewards4 / experiments\n",
    "\n",
    "# Comparison of Optimal Action percentage for the four experiment environments\n",
    "steps = np.arange(1, rounds + 1, 1)\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(steps, avgPercentage1, color = \"blue\", label = r'$\\varepsilon = 0.1, Q_1 = 0, eGreedy$')\n",
    "plt.plot(steps, avgPercentage2, color = \"green\", label = r'$\\varepsilon = 0.1, Q_1 = 5, eGreedy$')\n",
    "plt.plot(steps, avgPercentage3, color = \"red\", label = r'$c = 2, Q_1 = 0, UCB$')\n",
    "plt.plot(steps, avgPercentage4, color = \"yellow\", label = r'$\\alpha = 0.1, gradientAscent$')\n",
    "plt.xlabel(\"Total Rounds\", fontsize = 14)\n",
    "plt.ylabel(\"Average Optimal Action Percentage\", fontsize = 14)\n",
    "plt.title(f\"Effect of Initial Estimates on Optimal Action Percentage\", fontsize = 15, fontweight = \"bold\")\n",
    "plt.legend(fontsize = 14)\n",
    "plt.savefig(\"OptActionPercentage.png\")\n",
    "\n",
    "# Comparison of Average Reward for the four experiment environments \n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(steps, avgRewards1, color = \"blue\", label = r'$\\varepsilon = 0.1, Q_1 = 0, eGreedy$')\n",
    "plt.plot(steps, avgRewards2, color = \"green\", label = r'$\\varepsilon = 0.1, Q_1 = 5, eGreedy$')\n",
    "plt.plot(steps, avgRewards3, color = \"red\", label = r'$c = 2, Q_1 = 0, UCB$')\n",
    "plt.plot(steps, avgRewards4, color = \"yellow\", label = r'$\\alpha = 0.1, gradientAscent$')\n",
    "plt.xlabel(\"Total Rounds\", fontsize = 14)\n",
    "plt.ylabel(\"Average Reward\", fontsize = 14)\n",
    "plt.title(f\"Effect of Initial Estimates on Average Reward\", fontsize = 15, fontweight = \"bold\")\n",
    "plt.legend(fontsize = 14)\n",
    "plt.savefig(\"AvgReward.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
